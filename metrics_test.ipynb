{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of src.motion_refiner_4D failed: Traceback (most recent call last):\n",
      "  File \"/home/arthur/.conda/envs/py38_cu11_2/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 257, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/arthur/.conda/envs/py38_cu11_2/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 455, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/arthur/.conda/envs/py38_cu11_2/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/arthur/catkin_ws/src/NL_trajectory_reshaper/src/motion_refiner_4D.py\", line 14, in <module>\n",
      "    from config import *\n",
      "ModuleNotFoundError: No module named 'config'\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading CLIP model... done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from src.motion_refiner_4D import Motion_refiner\n",
    "from src.config import *\n",
    "# base_folder = \"/home/tum/data/\"\n",
    "# base_folder = \"/home/arthur/local_data/\" \n",
    "\n",
    "traj_n = 40\n",
    "mr = Motion_refiner(load_models=True ,traj_n = traj_n, locality_factor=True, clip_only=True)\n",
    "feature_indices, obj_sim_indices, obj_poses_indices, traj_indices = mr.get_indices()\n",
    "embedding_indices = np.concatenate([feature_indices,obj_sim_indices, obj_poses_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset:  4D_100k_scaping_factor ...done\n",
      "raw X: (100000, 953) \tY: (100000, 160)\n",
      "filtered X: (99092, 953) \tY: (99092, 160)\n",
      "ATTENTION!!! dataset and MR indexes dont match!!!\n",
      "motion refiner final index =  697\n",
      "dataset final index =  953\n",
      "Train X: (69363, 953) \tY: (69363, 160)\n",
      "Test  X: (19819, 953) \tY: (19819, 160)\n",
      "Val   X: (9910, 953) \tY: (9910, 160)\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"4D_100k_scaping_factor\"\n",
    "\n",
    "X,Y, data = mr.load_dataset(dataset_name, filter_data = True, base_path=base_folder+\"data/\")\n",
    "X_train, X_test, X_valid, y_train, y_test, y_valid, indices_train, indices_test, indices_val = mr.split_dataset(X, Y, test_size=0.2, val_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99092it [00:00, 687159.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395345\n",
      "The unique elements of the input list using set():\n",
      "\n",
      "9290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "change_img_base=['/mnt/tumdata/image_dataset',image_dataset_folder]\n",
    "\n",
    "all_image_paths = []\n",
    "for i, d in tqdm(enumerate(data)):\n",
    "    image_paths = d[\"image_paths\"]\n",
    "\n",
    "    if not change_img_base is None:\n",
    "        for ti in range(len(image_paths)):\n",
    "            all_image_paths.append(image_paths[ti].replace(change_img_base[0], change_img_base[1]))\n",
    "    \n",
    "print(len(all_image_paths))\n",
    "set_res = set(all_image_paths) \n",
    "print(\"The unique elements of the input list using set():\\n\") \n",
    "all_image_paths = (list(set_res))\n",
    "print(len(all_image_paths))\n",
    "\n",
    "# for item in list_res: \n",
    "#     print(item) \n",
    "# img_paths = [dd[\"image_paths\"] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1858\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "obj_library = {}\n",
    "with open(image_dataset_folder+\"imagenet1000_clsidx_to_labels.txt\") as f:\n",
    "    obj_library = json.load(f)\n",
    "\n",
    "obj_names = []\n",
    "for k,v in obj_library.items():\n",
    "    obj_names= obj_names+obj_library[k] \n",
    "print(len(obj_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "\n",
    "# def load_image(img_paths):\n",
    "#     img = \n",
    "#     return img\n",
    "\n",
    "def image_loader(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        b = lst[i:i + n]\n",
    "        classes = [c.split(\"/\")[-2] for c in b]\n",
    "        images = torch.concat([preprocess(Image.open(im)).unsqueeze(0) for im in b])\n",
    "        yield classes,images\n",
    "def get_image_features(image_paths):\n",
    "    all_features = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for classes, images in image_loader(image_paths,100):\n",
    "            print(images.shape)\n",
    "            features = model.encode_image(images.to(device))\n",
    "            all_features.append(features)\n",
    "    return torch.cat(all_features).cpu().numpy()\n",
    "\n",
    "def text_loader(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield clip.tokenize(lst[i:i + n]).to(device)\n",
    "def get_text_features(text):\n",
    "    all_features = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t in text_loader(text,100):\n",
    "            features = model.encode_text(t.to(device))\n",
    "            all_features.append(features)\n",
    "    return torch.cat(all_features).cpu().numpy()\n",
    "\n",
    "all_obj_names_features = get_text_features(obj_names)\n",
    "\n",
    "# all_features = get_image_features(all_image_paths)\n",
    "# all_features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "obj_names_clip_emb = {}\n",
    "for f, p in zip(all_obj_names_features,obj_names):\n",
    "    obj_names_clip_emb[p] = f.tolist()\n",
    "\n",
    "with open(image_dataset_folder+'obj_names_clip_emb.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(obj_names_clip_emb, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "images_clip_emb = {}\n",
    "for f, p in zip(all_features,all_image_paths):\n",
    "    images_clip_emb[p] = f.tolist()\n",
    "    \n",
    "with open(image_dataset_folder+'images_clip_emb.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(images_clip_emb, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'load_precomp_emb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/arthur/catkin_ws/src/NL_trajectory_reshaper/metrics_test.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arthur/catkin_ws/src/NL_trajectory_reshaper/metrics_test.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmotion_refiner_4D\u001b[39;00m \u001b[39mimport\u001b[39;00m Motion_refiner\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/arthur/catkin_ws/src/NL_trajectory_reshaper/metrics_test.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m mr \u001b[39m=\u001b[39m Motion_refiner(traj_n \u001b[39m=\u001b[39;49m \u001b[39m40\u001b[39;49m, clip_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, locality_factor\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, poses_on_features\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, load_precomp_emb\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'load_precomp_emb'"
     ]
    }
   ],
   "source": [
    "from src.motion_refiner_4D import Motion_refiner\n",
    "\n",
    "mr = Motion_refiner(traj_n = 40, clip_only=True, locality_factor=True, poses_on_features=True, load_precomp_emb=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_,Y_ = mr.prepare_data(data,deltas=False, change_img_base=['/mnt/tumdata/image_dataset',args.image_dataset_dir])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9290\n"
     ]
    }
   ],
   "source": [
    "images_clip_emb2={}\n",
    "with open(image_dataset_folder+\"images_clip_emb.json\") as f:\n",
    "    images_clip_emb2 = json.load(f)\n",
    "print(len(images_clip_emb2.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_features  (5, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_clip_features  torch.Size([1, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:38, 22.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_clip_features  torch.Size([1, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:03, 31.55s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/arthur/catkin_ws/src/NL_trajectory_reshaper/metrics_test.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/arthur/catkin_ws/src/NL_trajectory_reshaper/metrics_test.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m X,Y \u001b[39m=\u001b[39m mr\u001b[39m.\u001b[39;49mprepare_data(data[:\u001b[39m5\u001b[39;49m],deltas\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, change_img_base\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39m/mnt/tumdata/image_dataset\u001b[39;49m\u001b[39m'\u001b[39;49m,image_dataset_folder])\n",
      "File \u001b[0;32m~/catkin_ws/src/NL_trajectory_reshaper/src/motion_refiner_4D.py:349\u001b[0m, in \u001b[0;36mMotion_refiner.prepare_data\u001b[0;34m(self, data, deltas, label, interpolation, verbose, change_img_base)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[39mfor\u001b[39;00m ti \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(image_paths)):\n\u001b[1;32m    347\u001b[0m         image_paths[ti] \u001b[39m=\u001b[39m image_paths[ti]\u001b[39m.\u001b[39mreplace(change_img_base[\u001b[39m0\u001b[39m], change_img_base[\u001b[39m1\u001b[39m])\n\u001b[0;32m--> 349\u001b[0m d[\u001b[39m\"\u001b[39m\u001b[39mobj_names_features\u001b[39m\u001b[39m\"\u001b[39m], d[\u001b[39m\"\u001b[39m\u001b[39mtext_clip_features\u001b[39m\u001b[39m\"\u001b[39m],d[\u001b[39m\"\u001b[39m\u001b[39mimage_features\u001b[39m\u001b[39m\"\u001b[39m], d[\u001b[39m'\u001b[39m\u001b[39msimilarity\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_clip_similarity(\n\u001b[1;32m    350\u001b[0m     d[\u001b[39m\"\u001b[39;49m\u001b[39mobj_names\u001b[39;49m\u001b[39m\"\u001b[39;49m], [d[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m]], images_path \u001b[39m=\u001b[39;49m image_paths)\n\u001b[1;32m    352\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_only:\n\u001b[1;32m    353\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtext_clip_features \u001b[39m\u001b[39m\"\u001b[39m,d[\u001b[39m\"\u001b[39m\u001b[39mtext_clip_features\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/catkin_ws/src/NL_trajectory_reshaper/src/motion_refiner_4D.py:199\u001b[0m, in \u001b[0;36mMotion_refiner.compute_clip_similarity\u001b[0;34m(self, obj_names, text, images_path)\u001b[0m\n\u001b[1;32m    196\u001b[0m text_clip_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCLIP_model\u001b[39m.\u001b[39mencode_text(\n\u001b[1;32m    197\u001b[0m     token_clip_text)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m    198\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m images_path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_image(img_path) \u001b[39mfor\u001b[39;00m img_path \u001b[39min\u001b[39;00m images_path]\n\u001b[1;32m    200\u001b[0m     image_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(torch\u001b[39m.\u001b[39mcat(images))\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    201\u001b[0m     image_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCLIP_model\u001b[39m.\u001b[39mencode_image(image_input)\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/catkin_ws/src/NL_trajectory_reshaper/src/motion_refiner_4D.py:199\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    196\u001b[0m text_clip_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCLIP_model\u001b[39m.\u001b[39mencode_text(\n\u001b[1;32m    197\u001b[0m     token_clip_text)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m    198\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m images_path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_image(img_path) \u001b[39mfor\u001b[39;00m img_path \u001b[39min\u001b[39;00m images_path]\n\u001b[1;32m    200\u001b[0m     image_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(torch\u001b[39m.\u001b[39mcat(images))\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    201\u001b[0m     image_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCLIP_model\u001b[39m.\u001b[39mencode_image(image_input)\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/catkin_ws/src/NL_trajectory_reshaper/src/motion_refiner_4D.py:146\u001b[0m, in \u001b[0;36mMotion_refiner.load_image\u001b[0;34m(self, img_path)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_image\u001b[39m(\u001b[39mself\u001b[39m, img_path):\n\u001b[0;32m--> 146\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCLIP_preprocess(Image\u001b[39m.\u001b[39;49mopen(img_path))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    147\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.conda/envs/py38_cu11_2/lib/python3.8/site-packages/PIL/Image.py:3092\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3089\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[1;32m   3091\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[0;32m-> 3092\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   3093\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   3095\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X,Y = mr.prepare_data(data[:5],deltas=False, change_img_base=['/mnt/tumdata/image_dataset',image_dataset_folder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %matplotlib qt\n",
    "# from src.functions import *\n",
    "# idx = random.choices(range(len(data)),k=3)\n",
    "# data_sample = list(np.array(data)[idx])\n",
    "# show_data4D(data_sample)\n",
    "# # print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic time warping (DTW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functions import *\n",
    "\n",
    "n = -1\n",
    "trajs_x = mr.prepare_x(X_valid)[:n,6:,:]\n",
    "trajs_y = list_to_wp_seq(y_valid,d=4)[:n,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcm:\t nan\n",
      "dfd:\t 0.14597155479503446\n",
      "area:\t 0.027609880103219818\n",
      "cl:\t nan\n",
      "dtw:\t 3.7486616869839575\n",
      "mae:\t 0.0276043962328285\n",
      "mse:\t 0.005053778124616486\n"
     ]
    }
   ],
   "source": [
    "import similaritymeasures\n",
    "def compute_metrics(trajs_x, trajs_y, filter_nan=False):\n",
    "    \"\"\"Computes the similarity metrics between 2 trajectories:\n",
    "    returns: dict(pcm,df,area,cl,dtw,mae,mse), metrics over each sample\"\"\"\n",
    "\n",
    "    metrics = {\"pcm\":None,\"dfd\":None,\"area\":None,\"cl\":None,\"dtw\":None,\"mae\":None,\"mse\":None}\n",
    "\n",
    "    metrics_h = np.zeros((trajs_x.shape[0],7))\n",
    "    for i,(exp_data, num_data) in enumerate(zip(trajs_x, trajs_y)):\n",
    "\n",
    "        pcm = similaritymeasures.pcm(exp_data, num_data) # Partial Curve Mapping\n",
    "        dfd = similaritymeasures.frechet_dist(exp_data, num_data) # Discrete Frechet distance\n",
    "        area = similaritymeasures.area_between_two_curves(exp_data, num_data) # area between two curves\n",
    "        cl = similaritymeasures.curve_length_measure(exp_data, num_data)# Curve Length based similarity measure\n",
    "        dtw, d = similaritymeasures.dtw(exp_data, num_data) # Dynamic Time Warping distance\n",
    "        mae = similaritymeasures.mae(exp_data, num_data) # mean absolute error\n",
    "        mse = similaritymeasures.mse(exp_data, num_data) # mean squared error\n",
    "\n",
    "        metrics_h[i,:] = [pcm,dfd,area,cl,dtw,mae,mse]\n",
    "\n",
    "    if filter_nan:\n",
    "        metrics_h = metrics_h[~np.isnan(metrics_h).any(axis=1),:]\n",
    "    metrics_v = np.mean(metrics_h,axis=0)\n",
    "    for k,v in zip(metrics.keys(), metrics_v):\n",
    "        metrics[k]=v\n",
    "        print(k+\":\\t\",v)\n",
    "    return metrics, metrics_h\n",
    "\n",
    "metrics, metrics_h = compute_metrics(trajs_x, trajs_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38_cu11_2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5de91200c9f9e1f8a0c28ceba668014be0fd55838e84400e0a7ad1d269192773"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
