{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading BERT model... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "loading CLIP model... done\n",
      "DEVICE:  cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from src.motion_refiner_4D import Motion_refiner, MAX_NUM_OBJS\n",
    "from src.config import *\n",
    "# base_folder = \"/home/tum/data/\"\n",
    "# base_folder = \"/home/arthur/local_data/\" \n",
    "\n",
    "traj_n = 40\n",
    "mr = Motion_refiner(load_models=True ,traj_n = traj_n, locality_factor=True, clip_only=False)\n",
    "feature_indices, obj_sim_indices, obj_poses_indices, traj_indices = mr.get_indices()\n",
    "embedding_indices = np.concatenate([feature_indices,obj_sim_indices, obj_poses_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset:  4D_100k_scaping_factor ...done\n",
      "raw X: (100000, 953) \tY: (100000, 160)\n",
      "filtered X: (99092, 953) \tY: (99092, 160)\n",
      "Train X: (69363, 953) \tY: (69363, 160)\n",
      "Test  X: (19819, 953) \tY: (19819, 160)\n",
      "Val   X: (9910, 953) \tY: (9910, 160)\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"4D_100k_scaping_factor\"\n",
    "\n",
    "X,Y, data = mr.load_dataset(dataset_name, filter_data = True, base_path=base_folder+\"data/\")\n",
    "X_train, X_test, X_valid, y_train, y_test, y_valid, indices_train, indices_test, indices_val = mr.split_dataset(X, Y, test_size=0.2, val_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functions import *\n",
    "\n",
    "n = -4\n",
    "trajs_x = mr.prepare_x(X_test)[:n,MAX_NUM_OBJS:,:]\n",
    "trajs_y = list_to_wp_seq(y_test,d=4)[:n,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcm:\t nan\n",
      "dfd:\t 0.14606799108660493\n",
      "area:\t 0.027892437444718723\n",
      "cl:\t nan\n",
      "dtw:\t 3.789714883393604\n",
      "mae:\t 0.02784615995780797\n",
      "mse:\t 0.005119454187622462\n"
     ]
    }
   ],
   "source": [
    "metrics, metrics_h = compute_metrics(trajs_x, trajs_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99092it [00:00, 687159.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395345\n",
      "The unique elements of the input list using set():\n",
      "\n",
      "9290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "change_img_base=['/mnt/tumdata/image_dataset',image_dataset_folder]\n",
    "\n",
    "all_image_paths = []\n",
    "for i, d in tqdm(enumerate(data)):\n",
    "    image_paths = d[\"image_paths\"]\n",
    "\n",
    "    if not change_img_base is None:\n",
    "        for ti in range(len(image_paths)):\n",
    "            all_image_paths.append(image_paths[ti].replace(change_img_base[0], change_img_base[1]))\n",
    "    \n",
    "print(len(all_image_paths))\n",
    "set_res = set(all_image_paths) \n",
    "print(\"The unique elements of the input list using set():\\n\") \n",
    "all_image_paths = (list(set_res))\n",
    "print(len(all_image_paths))\n",
    "\n",
    "# for item in list_res: \n",
    "#     print(item) \n",
    "# img_paths = [dd[\"image_paths\"] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1858\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "obj_library = {}\n",
    "with open(image_dataset_folder+\"imagenet1000_clsidx_to_labels.txt\") as f:\n",
    "    obj_library = json.load(f)\n",
    "\n",
    "obj_names = []\n",
    "for k,v in obj_library.items():\n",
    "    obj_names= obj_names+obj_library[k] \n",
    "print(len(obj_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "\n",
    "# def load_image(img_paths):\n",
    "#     img = \n",
    "#     return img\n",
    "\n",
    "def image_loader(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        b = lst[i:i + n]\n",
    "        classes = [c.split(\"/\")[-2] for c in b]\n",
    "        images = torch.concat([preprocess(Image.open(im)).unsqueeze(0) for im in b])\n",
    "        yield classes,images\n",
    "def get_image_features(image_paths):\n",
    "    all_features = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for classes, images in image_loader(image_paths,100):\n",
    "            print(images.shape)\n",
    "            features = model.encode_image(images.to(device))\n",
    "            all_features.append(features)\n",
    "    return torch.cat(all_features).cpu().numpy()\n",
    "\n",
    "def text_loader(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield clip.tokenize(lst[i:i + n]).to(device)\n",
    "def get_text_features(text):\n",
    "    all_features = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t in text_loader(text,100):\n",
    "            features = model.encode_text(t.to(device))\n",
    "            all_features.append(features)\n",
    "    return torch.cat(all_features).cpu().numpy()\n",
    "\n",
    "all_obj_names_features = get_text_features(obj_names)\n",
    "\n",
    "# all_features = get_image_features(all_image_paths)\n",
    "# all_features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "obj_names_clip_emb = {}\n",
    "for f, p in zip(all_obj_names_features,obj_names):\n",
    "    obj_names_clip_emb[p] = f.tolist()\n",
    "\n",
    "with open(image_dataset_folder+'obj_names_clip_emb.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(obj_names_clip_emb, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "images_clip_emb = {}\n",
    "for f, p in zip(all_features,all_image_paths):\n",
    "    images_clip_emb[p] = f.tolist()\n",
    "    \n",
    "with open(image_dataset_folder+'images_clip_emb.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(images_clip_emb, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'load_precomp_emb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/arthur/catkin_ws/src/latte/metrics_test.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arthur/catkin_ws/src/latte/metrics_test.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmotion_refiner_4D\u001b[39;00m \u001b[39mimport\u001b[39;00m Motion_refiner\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/arthur/catkin_ws/src/latte/metrics_test.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m mr \u001b[39m=\u001b[39m Motion_refiner(traj_n \u001b[39m=\u001b[39;49m \u001b[39m40\u001b[39;49m, clip_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, locality_factor\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, poses_on_features\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, load_precomp_emb\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'load_precomp_emb'"
     ]
    }
   ],
   "source": [
    "from src.motion_refiner_4D import Motion_refiner\n",
    "\n",
    "mr = Motion_refiner(traj_n = 40, clip_only=True, locality_factor=True, poses_on_features=True, load_precomp_emb=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_,Y_ = mr.prepare_data(data,deltas=False, change_img_base=['/mnt/tumdata/image_dataset',args.image_dataset_dir])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9290\n"
     ]
    }
   ],
   "source": [
    "images_clip_emb2={}\n",
    "with open(image_dataset_folder+\"images_clip_emb.json\") as f:\n",
    "    images_clip_emb2 = json.load(f)\n",
    "print(len(images_clip_emb2.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_features  (5, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_clip_features  torch.Size([1, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:38, 22.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_clip_features  torch.Size([1, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:03, 31.55s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/arthur/catkin_ws/src/latte/metrics_test.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/arthur/catkin_ws/src/latte/metrics_test.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m X,Y \u001b[39m=\u001b[39m mr\u001b[39m.\u001b[39;49mprepare_data(data[:\u001b[39m5\u001b[39;49m],deltas\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, change_img_base\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39m/mnt/tumdata/image_dataset\u001b[39;49m\u001b[39m'\u001b[39;49m,image_dataset_folder])\n",
      "File \u001b[0;32m~/catkin_ws/src/latte/src/motion_refiner_4D.py:349\u001b[0m, in \u001b[0;36mMotion_refiner.prepare_data\u001b[0;34m(self, data, deltas, label, interpolation, verbose, change_img_base)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[39mfor\u001b[39;00m ti \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(image_paths)):\n\u001b[1;32m    347\u001b[0m         image_paths[ti] \u001b[39m=\u001b[39m image_paths[ti]\u001b[39m.\u001b[39mreplace(change_img_base[\u001b[39m0\u001b[39m], change_img_base[\u001b[39m1\u001b[39m])\n\u001b[0;32m--> 349\u001b[0m d[\u001b[39m\"\u001b[39m\u001b[39mobj_names_features\u001b[39m\u001b[39m\"\u001b[39m], d[\u001b[39m\"\u001b[39m\u001b[39mtext_clip_features\u001b[39m\u001b[39m\"\u001b[39m],d[\u001b[39m\"\u001b[39m\u001b[39mimage_features\u001b[39m\u001b[39m\"\u001b[39m], d[\u001b[39m'\u001b[39m\u001b[39msimilarity\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_clip_similarity(\n\u001b[1;32m    350\u001b[0m     d[\u001b[39m\"\u001b[39;49m\u001b[39mobj_names\u001b[39;49m\u001b[39m\"\u001b[39;49m], [d[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m]], images_path \u001b[39m=\u001b[39;49m image_paths)\n\u001b[1;32m    352\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_only:\n\u001b[1;32m    353\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtext_clip_features \u001b[39m\u001b[39m\"\u001b[39m,d[\u001b[39m\"\u001b[39m\u001b[39mtext_clip_features\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/catkin_ws/src/latte/src/motion_refiner_4D.py:199\u001b[0m, in \u001b[0;36mMotion_refiner.compute_clip_similarity\u001b[0;34m(self, obj_names, text, images_path)\u001b[0m\n\u001b[1;32m    196\u001b[0m text_clip_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCLIP_model\u001b[39m.\u001b[39mencode_text(\n\u001b[1;32m    197\u001b[0m     token_clip_text)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m    198\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m images_path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_image(img_path) \u001b[39mfor\u001b[39;00m img_path \u001b[39min\u001b[39;00m images_path]\n\u001b[1;32m    200\u001b[0m     image_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(torch\u001b[39m.\u001b[39mcat(images))\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    201\u001b[0m     image_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCLIP_model\u001b[39m.\u001b[39mencode_image(image_input)\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/catkin_ws/src/latte/src/motion_refiner_4D.py:199\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    196\u001b[0m text_clip_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCLIP_model\u001b[39m.\u001b[39mencode_text(\n\u001b[1;32m    197\u001b[0m     token_clip_text)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m    198\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m images_path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_image(img_path) \u001b[39mfor\u001b[39;00m img_path \u001b[39min\u001b[39;00m images_path]\n\u001b[1;32m    200\u001b[0m     image_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(torch\u001b[39m.\u001b[39mcat(images))\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    201\u001b[0m     image_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCLIP_model\u001b[39m.\u001b[39mencode_image(image_input)\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/catkin_ws/src/latte/src/motion_refiner_4D.py:146\u001b[0m, in \u001b[0;36mMotion_refiner.load_image\u001b[0;34m(self, img_path)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_image\u001b[39m(\u001b[39mself\u001b[39m, img_path):\n\u001b[0;32m--> 146\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCLIP_preprocess(Image\u001b[39m.\u001b[39;49mopen(img_path))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    147\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.conda/envs/py38_cu11_2/lib/python3.8/site-packages/PIL/Image.py:3092\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3089\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[1;32m   3091\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[0;32m-> 3092\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   3093\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   3095\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X,Y = mr.prepare_data(data[:5],deltas=False, change_img_base=['/mnt/tumdata/image_dataset',image_dataset_folder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic time warping (DTW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 1 4]\n",
      "[2 3 4 1]\n",
      "matmul\n",
      "-----------------\n",
      "[2 3]\n",
      "[[0.0332502946 0 0]\n",
      " [0.0832713246 0 0]]\n"
     ]
    }
   ],
   "source": [
    "dtype = tf.float32\n",
    "y_true =  tf.constant([[[1.0, 1.0, 1.0, 1.0],[1.0, 1.0, 1.0, 1.0],[1.0, 1.0, 1.0, 1.0]],\n",
    "                        [[1.0, 1.0, 1.0, 1.0],[1.0, 1.0, 1.0, 1.0],[1.0, 1.0, 1.0, 1.0]]],dtype=dtype)\n",
    "y_pred =  tf.constant([[[0.5, 1.0, 1.0, 1.0],[1.0, 1.0, 1.0, 1.0],[1.0, 1.0, 1.0, 1.0]],\n",
    "                        [[2.0, 1.0, 1.0, 1.0],[1.0, 1.0, 1.0, 1.0],[1.0, 1.0, 1.0, 1.0]]],dtype=dtype)\n",
    "\n",
    "y_true_abs = tf.norm(y_true, ord='euclidean', axis=-1)\n",
    "y_pred_abs = tf.norm(y_pred, ord='euclidean', axis=-1)\n",
    "\n",
    "y_true = tf.expand_dims(y_true, -1)\n",
    "y_pred = tf.expand_dims(y_pred, -1)\n",
    "\n",
    "y_pred  = tf.transpose(y_pred, [0, 1, 3,2])\n",
    "\n",
    "tf.print(tf.shape(y_pred))\n",
    "tf.print(tf.shape(y_true))\n",
    "\n",
    "a = tf.squeeze(tf.matmul(y_pred, y_true),[2, 3])\n",
    "b = tf.multiply(y_true_abs,y_pred_abs)\n",
    "angle_diff = tf.add(1.0,-tf.math.divide(a,b))\n",
    "lenght_diff = tf.math.divide(tf.math.sqrt(tf.square(y_true_abs-y_pred_abs)),tf.add(y_true_abs,y_pred_abs))\n",
    "\n",
    "k1 = 0.5\n",
    "k2 = 0.5\n",
    "\n",
    "loss_LDA = tf.add(tf.math.multiply(k1,lenght_diff), tf.math.multiply(k2*1/2,angle_diff))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38_cu11_2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5de91200c9f9e1f8a0c28ceba668014be0fd55838e84400e0a7ad1d269192773"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
